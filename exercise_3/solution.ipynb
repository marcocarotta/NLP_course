{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Precision of a Classifier\n",
    "\n",
    "The precision of a classifier can be calculated using the formula:\n",
    "\n",
    "$$\t\\text{Precision} = \\frac{\t\\text{True Positives}}{\t\\text{True Positives} + \t\\text{False Positives}}$$\n",
    "\n",
    "## Calculation\n",
    "We will assume the following layout for the confusion matrix: the cell in position $i,j$ in the matrix indicates the count of instances of class $i$ classificated as class $j$.\n",
    "\n",
    "### Identify the true positives and false positives.\n",
    "For each class, precision is calculated as:\n",
    "\n",
    "$$\\text{Precision}_i = \\frac{\\text{True Positives for Class } i}{\\text{True Positives for Class } i + \\text{False Positives for Class } i}$$\n",
    "\n",
    "- **True Positives (TP)**: Values along the diagonal of the matrix.\n",
    "- **False Positives (FP)**: Sum of the values in the respective column, excluding the diagonal value.\n",
    "\n",
    "### Calculate precision for each class.\n",
    "For each class, find:\n",
    "\n",
    "1. **Class 1:**\n",
    "   - TP = 97\n",
    "   - FP = 28 + 71 + 14 + 42 = 155\n",
    "   - $\\text{Precision}_1 = \\frac{97}{97 + 155} \\approx 0.384$\n",
    "\n",
    "2. **Class 2:**\n",
    "   - TP = 13\n",
    "   - FP = 14 + 9 + 8 + 15 = 46\n",
    "   - $\\text{Precision}_2 = \\frac{13}{13 + 46} \\approx 0.220$\n",
    "\n",
    "3. **Class 3:**\n",
    "   - TP = 92\n",
    "   - FP = 45 + 31 + 22 + 42 = 140\n",
    "   - $\\text{Precision}_3 = \\frac{92}{92 + 140} \\approx 0.396$\n",
    "\n",
    "4. **Class 4:**\n",
    "   - TP = 184\n",
    "   - FP = 63 + 85 + 47 + 58 = 253\n",
    "   - $\\text{Precision}_4 = \\frac{184}{184 + 253} \\approx 0.421$\n",
    "\n",
    "5. **Class 5:**\n",
    "   - TP = 187\n",
    "   - FP = 83 + 93 + 115 + 34 = 325\n",
    "   - $\\text{Precision}_5 = \\frac{187}{187 + 325} \\approx 0.365$\n",
    "\n",
    "### Calculate the overall precision.\n",
    "If the precision values are averaged equally for all classes, that is using a macro averaging, then:\n",
    "\n",
    "$\\text{Macro-Averaging Precision} = \\frac{0.384 + 0.220 + 0.396 + 0.421 + 0.365}{5} \\approx 0.357$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: BLEU score\n",
    "In the following we are going to implement a bleu scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1, tokenized text and reference\n",
      "\n",
      "Text:  ['you', 'are', 'four', 'minutes', 'late']\n",
      "Reference:  ['you', 'are', 'four', 'minutes', 'too', 'slow']\n",
      "Implemented bleu score:  [0.8, 0.75, 0.6666666666666666, 0.5]\n",
      "True bleu score:  [0.8, 0.75, 0.6666666666666666, 0.5] \n",
      "\n",
      "Example 2, tokenized text and reference\n",
      "\n",
      "Text:  ['what', 'was', 'certain', 'however', 'was', 'that', 'Phileas', 'Fogg', 'had', 'not', 'left', 'London', 'for', 'many', 'years']\n",
      "Reference:  ['it', 'was', 'at', 'least', 'certain', 'that', 'Phileas', 'Fogg', 'had', 'not', 'absented', 'himself', 'from', 'London', 'for', 'many', 'years']\n",
      "Implemented bleu score:  [0.8, 0.5, 0.38461538461538464, 0.25]\n",
      "True bleu score:  [0.7333333333333333, 0.5, 0.38461538461538464, 0.25]\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def bleu_score(text:list[str] ,reference:list[str], n) -> list[float]:\n",
    "    \"\"\" BLEU score calculation, the implementation only consider if the n-gram is present in the reference and not how many times it is present\n",
    "    Args:\n",
    "        text: list of tokenized strings, it is the prediction to evaluate\n",
    "        reference: list of tokenized strings, it is the reference to compare the prediction with\n",
    "        n: size of the biggest n-gram to use for evaluation\n",
    "    Returns:\n",
    "        list of size n of BLEU scores for each n-gram size, in order from 1 to n\n",
    "    \"\"\"\n",
    "    if len(text) < n:\n",
    "        raise ValueError(\"The text is too short to calculate the BLEU score with the given n-gram size\")\n",
    "    # Calculate the BLEU score for each n-gram size\n",
    "    bleu_scores = []\n",
    "    print(\"Text: \",text)\n",
    "    print(\"Reference: \",reference)\n",
    "    for i in range(1, n+1):\n",
    "        # Calculate the precision for the current n-gram size\n",
    "        precision = 0\n",
    "        for j in range(len(text) - i + 1):\n",
    "            ngram = text[j:j+i]\n",
    "            if ngram in [reference[k:k+i] for k in range(len(reference) - i + 1)]:\n",
    "                precision += 1\n",
    "        precision /= len(text) - i + 1\n",
    "        bleu_scores.append(precision)\n",
    "    return bleu_scores\n",
    "\n",
    "\n",
    "# example 1 of testing the BLEU score\n",
    "print(\"Example 1, tokenized text and reference\\n\")\n",
    "\n",
    "references = \"you are four minutes too slow\"\n",
    "predictions = \"you are four minutes late\"\n",
    "\n",
    "# Calculate the BLEU score from the implementation\n",
    "bleu_scores = bleu_score(tokenizer(predictions), tokenizer(references), 4)\n",
    "print(\"Implemented bleu score: \",bleu_scores)\n",
    "\n",
    "# Calculate the BLEU score from a library \n",
    "bleu = evaluate.load(\"bleu\")\n",
    "true_bleu = bleu.compute(predictions = [predictions], references = [references])\n",
    "print(\"True bleu score: \",true_bleu['precisions'],\"\\n\")\n",
    "\n",
    "\n",
    "# example 2 of testing the BLEU score\n",
    "print(\"Example 2, tokenized text and reference\\n\")\n",
    "\n",
    "ref2 = \"it was at least certain that Phileas Fogg had not absented himself from London for many years\"\n",
    "pred2 = \"what was certain however was that Phileas Fogg had not left London for many years\"\n",
    "\n",
    "# Calculate the BLEU score from the implementation \n",
    "bleu_scores = bleu_score(tokenizer(pred2), tokenizer(ref2), 4)\n",
    "print(\"Implemented bleu score: \",bleu_scores)\n",
    "\n",
    "# Calculate the BLEU score from a library\n",
    "true_bleu = bleu.compute(predictions = [pred2], references = [ref2])\n",
    "print(\"True bleu score: \",true_bleu['precisions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still satisfied with the results of our implementation, even though we get a slightly different set of results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
